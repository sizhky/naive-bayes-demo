{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "step-0-data-inspection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfFcXAgTMAsMpRB5BmcHeB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sizhky/naive-bayes-demo/blob/main/spam-ham/step_0_data_inspection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-ciqa4--dBX"
      },
      "source": [
        "%%capture\n",
        "!pip install torch_snippets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ol0YXrc1uaTl",
        "outputId": "e295e708-c150-4377-c1fb-5347dce34aa6"
      },
      "source": [
        "## Setup and imports\n",
        "import os\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_snippets import unzip_file"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-29 05:57:58.436 | WARNING  | torch_snippets.torch_loader:<module>:233 - Not importing Lightning Report\n",
            "2021-07-29 05:57:58.444 | WARNING  | torch_snippets:<module>:13 - sklearn is not found. Skipping relevant imports from submodule `sklegos`\n",
            "Exception: No module named 'sklego'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWZ8DXXp2aCi"
      },
      "source": [
        "if not os.path.exists('SMSSpamCollection'):\n",
        "    urllib.request.urlretrieve (\"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\", \"smsspamcollection.zip\")\n",
        "    # Extracting the dataset\n",
        "    unzip_file('smsspamcollection.zip', './')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k9CuOar2XZc"
      },
      "source": [
        "df = pd.read_csv('SMSSpamCollection', sep='\\t', header=None)\n",
        "df.columns = ['class','content']"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdQXy6qe4NPj"
      },
      "source": [
        "from collections import Counter\n",
        "def get_word_count(sentences):\n",
        "    words = [word for sentence in sentences for word in sentence.split()]\n",
        "    return Counter(words)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikljwUYBzqit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c471912f-cb48-4944-8ac1-806faa8374c7"
      },
      "source": [
        "ham_sentences = df[df['class'] == 'ham'].content.tolist()\n",
        "ham_word_count = get_word_count(ham_sentences)\n",
        "print(\"HAM\")\n",
        "print(ham_word_count.most_common(20))\n",
        "\n",
        "spam_sentences = df[df['class'] == 'spam'].content.tolist()\n",
        "spam_word_count = get_word_count(spam_sentences)\n",
        "print(\"SPAM\")\n",
        "print(spam_word_count.most_common(20))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HAM\n",
            "[('to', 1538), ('you', 1462), ('I', 1439), ('the', 1029), ('a', 977), ('i', 742), ('and', 739), ('in', 736), ('u', 651), ('is', 645), ('my', 621), ('me', 541), ('of', 499), ('for', 481), ('that', 399), ('it', 376), ('your', 374), ('on', 352), ('have', 349), ('at', 334)]\n",
            "SPAM\n",
            "[('to', 607), ('a', 360), ('your', 187), ('call', 185), ('or', 185), ('the', 178), ('2', 169), ('for', 169), ('you', 164), ('is', 143), ('Call', 136), ('on', 136), ('have', 128), ('and', 119), ('from', 116), ('ur', 107), ('with', 101), ('&', 98), ('4', 93), ('of', 93)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QATedGQd65uN"
      },
      "source": [
        "Both lists are giving similar top words. \n",
        "Let's remove the common words to see if the classes have different set of most_common words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EagMIk8m64LH",
        "outputId": "1e211a0b-dae1-46ce-8eea-1480b54757cb"
      },
      "source": [
        "def remove_common_words(word_count, common_words):\n",
        "    for word in common_words:\n",
        "        del word_count[word]\n",
        "        \n",
        "all_words = get_word_count(df.content)\n",
        "common_words = all_words.most_common(50)\n",
        "common_words = [word for word,count in common_words]\n",
        "common_words[:10]"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['to', 'you', 'I', 'a', 'the', 'and', 'in', 'is', 'i', 'u']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrEoWmTQ3ixB",
        "outputId": "d0cdbdc1-9992-44bf-bbc2-29ee8aeaa5d6"
      },
      "source": [
        "ham_sentences = df[df['class'] == 'ham'].content.tolist()\n",
        "ham_word_count = get_word_count(ham_sentences)\n",
        "remove_common_words(ham_word_count, common_words)\n",
        "print(\"HAM\")\n",
        "print(ham_word_count.most_common(20))\n",
        "\n",
        "spam_sentences = df[df['class'] == 'spam'].content.tolist()\n",
        "spam_word_count = get_word_count(spam_sentences)\n",
        "remove_common_words(spam_word_count, common_words)\n",
        "print(\"SPAM\")\n",
        "print(spam_word_count.most_common(20))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HAM\n",
            "[('got', 200), ('come', 198), ('all', 193), ('was', 191), ('?', 181), ('am', 176), ('out', 167), ('...', 162), ('about', 143), ('want', 142), ('going', 141), ('then', 138), (\"I'll\", 138), ('time', 138), ('need', 136), ('How', 132), ('n', 131), ('But', 131), ('what', 131), ('still', 129)]\n",
            "SPAM\n",
            "[('Call', 136), ('&', 98), ('FREE', 89), ('mobile', 81), ('our', 76), ('To', 73), ('claim', 73), ('Your', 71), ('txt', 68), ('text', 68), ('now', 64), ('Txt', 63), ('reply', 58), ('free', 56), ('contact', 56), ('-', 55), ('now!', 49), ('send', 46), ('won', 45), ('only', 45)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nkSEJCl7WtT"
      },
      "source": [
        "Words like 'Your' are still coming as common words. This is likely because of capitalization. Let's lower case and retry"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEcHT9hm7Va-",
        "outputId": "f7a36058-a89e-4c76-9c6e-3a1bc70fad3c"
      },
      "source": [
        "df['content'] = df['content'].str.lower()\n",
        "\n",
        "all_words = get_word_count(df.content)\n",
        "common_words = all_words.most_common(50)\n",
        "common_words = [word for word,count in common_words]\n",
        "common_words[:10]\n",
        "\n",
        "ham_sentences = df[df['class'] == 'ham'].content.tolist()\n",
        "ham_word_count = get_word_count(ham_sentences)\n",
        "remove_common_words(ham_word_count, common_words)\n",
        "print(\"HAM\")\n",
        "print(ham_word_count.most_common(20))\n",
        "\n",
        "spam_sentences = df[df['class'] == 'spam'].content.tolist()\n",
        "spam_word_count = get_word_count(spam_sentences)\n",
        "remove_common_words(spam_word_count, common_words)\n",
        "print(\"SPAM\")\n",
        "print(spam_word_count.most_common(20))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HAM\n",
            "[('got', 228), ('like', 223), ('was', 221), ('come', 218), ('know', 208), ('am', 204), ('its', 203), ('then', 195), ('good', 189), ('?', 181), ('he', 180), ('out', 173), (\"i'll\", 168), ('...', 162), ('going', 157), ('Ã¼', 157), ('ok', 156), ('want', 154), ('love', 153), ('time', 153)]\n",
            "SPAM\n",
            "[('free', 180), ('txt', 136), ('text', 112), ('mobile', 109), ('claim', 106), ('reply', 101), ('&', 98), ('stop', 90), ('our', 85), ('now!', 71), ('new', 69), ('send', 66), ('only', 66), ('won', 64), ('nokia', 64), ('win', 58), ('prize', 58), ('cash', 56), ('contact', 56), ('-', 55)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soTrbPIp6dJG"
      },
      "source": [
        "# Inferences\n",
        "* Sentences should be lower cased\n",
        "* Common words should be removed \n",
        "* Words like free, claim, won, win, prize are clearly spammy words.\n",
        "* A simple naive bayes model should be able to separate spam from ham"
      ]
    }
  ]
}